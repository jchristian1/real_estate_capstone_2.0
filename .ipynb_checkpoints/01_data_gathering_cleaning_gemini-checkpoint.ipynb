{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5444a705-878a-49a9-99e7-9e6b6bb68cc8",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries\n",
    "First, let's import the libraries we'll need for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c21836-2788-43b1-b9a4-54e823ad6326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a7ff0-f920-4cad-a9aa-b2e85c04281e",
   "metadata": {},
   "source": [
    "## 2. Define File Paths and URL\n",
    "Next, we'll define the URL for the data and the names of the files we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca332bb-6577-4412-a54f-f9b3fd7d4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the Redfin data\n",
    "url = \"https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/zip_code_market_tracker.tsv000.gz\"\n",
    "\n",
    "# File names for the downloaded and processed data\n",
    "gzipped_file = 'data/raw/zip_code_market_tracker.tsv000.gz'\n",
    "unzipped_file = 'data/raw/zip_code_market_tracker.tsv'\n",
    "# CHANGED: The final raw file is now saved in the data/raw/ directory\n",
    "final_csv_file = 'data/raw/redfin_raw_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752a4bb-b33d-4a76-bda4-fcd361183e70",
   "metadata": {},
   "source": [
    "## 3. Download the Data\n",
    "This function will download the file from the URL. It includes a basic check to see if the download was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c1b155-4d29-4912-b75f-e9d4a1e031ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, filename):\n",
    "    \"\"\"Downloads a file from a given URL.\"\"\"\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(filename, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        print(f\"Successfully downloaded {filename}\")\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return False\n",
    "\n",
    "def decompress_gzip(gzipped_file, unzipped_file):\n",
    "    \"\"\"Decompresses a gzipped file.\"\"\"\n",
    "    try:\n",
    "        with gzip.open(gzipped_file, 'rb') as f_in:\n",
    "            with open(unzipped_file, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"Successfully decompressed {gzipped_file} to {unzipped_file}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error decompressing file: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa550836-095b-45c2-8232-fa69db692237",
   "metadata": {},
   "source": [
    "## 4. Load and Process the Data\n",
    "This is the core of the script. It orchestrates the downloading, loading, and merging of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae18e61a-7a35-4b94-ab97-97d2ed5b8b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data/raw/zip_code_market_tracker.tsv000.gz\n",
      "Successfully decompressed data/raw/zip_code_market_tracker.tsv000.gz to data/raw/zip_code_market_tracker.tsv\n",
      "Successfully loaded new data.\n",
      "No existing dataset found. Creating a new one.\n",
      "Dataset saved to redfin_zipcode_data.csv\n",
      "Cleaned up temporary files.\n"
     ]
    }
   ],
   "source": [
    "def update_dataset(url, gzipped_file, unzipped_file, final_csv_file):\n",
    "    \"\"\"\n",
    "    Downloads, processes, and updates the Redfin dataset.\n",
    "    \"\"\"\n",
    "    # ADDED: Create the data/raw directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(gzipped_file), exist_ok=True)\n",
    "    \n",
    "    if not download_file(url, gzipped_file):\n",
    "        return\n",
    "\n",
    "    if not decompress_gzip(gzipped_file, unzipped_file):\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        new_data = pd.read_csv(unzipped_file, sep='\\t')\n",
    "        print(\"Successfully loaded new data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return\n",
    "\n",
    "    if os.path.exists(final_csv_file):\n",
    "        print(\"Existing dataset found. Merging with new data.\")\n",
    "        old_data = pd.read_csv(final_csv_file)\n",
    "        combined_data = pd.concat([old_data, new_data], ignore_index=True)\n",
    "        # CHANGED: Column names are lowercase in the raw file\n",
    "        combined_data.drop_duplicates(subset=['region', 'period_begin', 'period_end'], keep='last', inplace=True)\n",
    "        print(\"Successfully merged and removed duplicates.\")\n",
    "    else:\n",
    "        print(\"No existing dataset found. Creating a new one.\")\n",
    "        combined_data = new_data\n",
    "\n",
    "    combined_data.to_csv(final_csv_file, index=False)\n",
    "    print(f\"Raw dataset saved to {final_csv_file}\")\n",
    "\n",
    "    # Clean up temporary downloaded files\n",
    "    os.remove(gzipped_file)\n",
    "    os.remove(unzipped_file)\n",
    "    print(\"Cleaned up temporary files.\")\n",
    "\n",
    "# --- Step 4: Run the Process ---\n",
    "update_dataset(url, gzipped_file, unzipped_file, final_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc772cfe-7e27-48d5-90f9-a2cd627f4b30",
   "metadata": {},
   "source": [
    "## 6. Load the Dataset\n",
    "Inspect the dataset it's shape, types,typos, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a07468a-a359-425e-86f6-a14a75d84401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "The dataset contains 9,127,639 rows and 58 columns.\n",
      "\n",
      "Most recent data is from: 2025-05-01\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('redfin_zipcode_data.csv')\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(f\"The dataset contains {df.shape[0]:,} rows and {df.shape[1]} columns.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'redfin_zipcode_data.csv' not found. Please run the data download script first.\")\n",
    "\n",
    "# --- Data Preparation ---\n",
    "\n",
    "# Convert date columns to datetime objects\n",
    "df['PERIOD_BEGIN'] = pd.to_datetime(df['PERIOD_BEGIN'])\n",
    "df['PERIOD_END'] = pd.to_datetime(df['PERIOD_END'])\n",
    "\n",
    "# For this analysis, we want the most recent data for each zip code.\n",
    "# Let's find the latest period available in the dataset.\n",
    "latest_date = df['PERIOD_BEGIN'].max()\n",
    "print(f\"\\nMost recent data is from: {latest_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Create a dataframe with only the most recent data for each zip code\n",
    "latest_df = df[df['PERIOD_BEGIN'] == latest_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2b44ba-8496-479c-bcf7-7554a04923d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERIOD_BEGIN</th>\n",
       "      <th>PERIOD_END</th>\n",
       "      <th>PERIOD_DURATION</th>\n",
       "      <th>REGION_TYPE</th>\n",
       "      <th>REGION_TYPE_ID</th>\n",
       "      <th>TABLE_ID</th>\n",
       "      <th>IS_SEASONALLY_ADJUSTED</th>\n",
       "      <th>REGION</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>...</th>\n",
       "      <th>SOLD_ABOVE_LIST_YOY</th>\n",
       "      <th>PRICE_DROPS</th>\n",
       "      <th>PRICE_DROPS_MOM</th>\n",
       "      <th>PRICE_DROPS_YOY</th>\n",
       "      <th>OFF_MARKET_IN_TWO_WEEKS</th>\n",
       "      <th>OFF_MARKET_IN_TWO_WEEKS_MOM</th>\n",
       "      <th>OFF_MARKET_IN_TWO_WEEKS_YOY</th>\n",
       "      <th>PARENT_METRO_REGION</th>\n",
       "      <th>PARENT_METRO_REGION_METRO_CODE</th>\n",
       "      <th>LAST_UPDATED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-02-01</td>\n",
       "      <td>2019-04-30</td>\n",
       "      <td>90</td>\n",
       "      <td>zip code</td>\n",
       "      <td>2</td>\n",
       "      <td>4106</td>\n",
       "      <td>False</td>\n",
       "      <td>Zip Code: 12210</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>Albany, NY</td>\n",
       "      <td>10580</td>\n",
       "      <td>2025-08-01 14:37:32.798 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>90</td>\n",
       "      <td>zip code</td>\n",
       "      <td>2</td>\n",
       "      <td>35899</td>\n",
       "      <td>False</td>\n",
       "      <td>Zip Code: 84075</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Utah</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ogden, UT</td>\n",
       "      <td>36260</td>\n",
       "      <td>2025-08-01 14:37:32.798 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-04-01</td>\n",
       "      <td>2024-06-30</td>\n",
       "      <td>90</td>\n",
       "      <td>zip code</td>\n",
       "      <td>2</td>\n",
       "      <td>4362</td>\n",
       "      <td>False</td>\n",
       "      <td>Zip Code: 12771</td>\n",
       "      <td>NaN</td>\n",
       "      <td>New York</td>\n",
       "      <td>...</td>\n",
       "      <td>0.186930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>-0.032258</td>\n",
       "      <td>0.027479</td>\n",
       "      <td>Poughkeepsie, NY</td>\n",
       "      <td>39100</td>\n",
       "      <td>2025-08-01 14:37:32.798 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>2013-05-31</td>\n",
       "      <td>90</td>\n",
       "      <td>zip code</td>\n",
       "      <td>2</td>\n",
       "      <td>5480</td>\n",
       "      <td>False</td>\n",
       "      <td>Zip Code: 15047</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Pittsburgh, PA</td>\n",
       "      <td>38300</td>\n",
       "      <td>2025-08-01 14:37:32.798 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>2013-05-31</td>\n",
       "      <td>90</td>\n",
       "      <td>zip code</td>\n",
       "      <td>2</td>\n",
       "      <td>6689</td>\n",
       "      <td>False</td>\n",
       "      <td>Zip Code: 17408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028571</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>York, PA</td>\n",
       "      <td>49620</td>\n",
       "      <td>2025-08-01 14:37:32.798 Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PERIOD_BEGIN PERIOD_END  PERIOD_DURATION REGION_TYPE  REGION_TYPE_ID  \\\n",
       "0   2019-02-01 2019-04-30               90    zip code               2   \n",
       "1   2019-10-01 2019-12-31               90    zip code               2   \n",
       "2   2024-04-01 2024-06-30               90    zip code               2   \n",
       "3   2013-03-01 2013-05-31               90    zip code               2   \n",
       "4   2013-03-01 2013-05-31               90    zip code               2   \n",
       "\n",
       "   TABLE_ID  IS_SEASONALLY_ADJUSTED           REGION  CITY         STATE  ...  \\\n",
       "0      4106                   False  Zip Code: 12210   NaN      New York  ...   \n",
       "1     35899                   False  Zip Code: 84075   NaN          Utah  ...   \n",
       "2      4362                   False  Zip Code: 12771   NaN      New York  ...   \n",
       "3      5480                   False  Zip Code: 15047   NaN  Pennsylvania  ...   \n",
       "4      6689                   False  Zip Code: 17408   NaN  Pennsylvania  ...   \n",
       "\n",
       "  SOLD_ABOVE_LIST_YOY PRICE_DROPS  PRICE_DROPS_MOM  PRICE_DROPS_YOY  \\\n",
       "0            0.000000         NaN              NaN              NaN   \n",
       "1                 NaN         NaN              NaN              NaN   \n",
       "2            0.186930         NaN              NaN              NaN   \n",
       "3            0.000000         NaN              NaN              NaN   \n",
       "4           -0.028571         NaN              NaN              NaN   \n",
       "\n",
       "   OFF_MARKET_IN_TWO_WEEKS  OFF_MARKET_IN_TWO_WEEKS_MOM  \\\n",
       "0                 1.000000                     0.666667   \n",
       "1                 1.000000                     0.500000   \n",
       "2                 0.064516                    -0.032258   \n",
       "3                 0.000000                     0.000000   \n",
       "4                 0.000000                     0.000000   \n",
       "\n",
       "   OFF_MARKET_IN_TWO_WEEKS_YOY  PARENT_METRO_REGION  \\\n",
       "0                     0.750000           Albany, NY   \n",
       "1                          NaN            Ogden, UT   \n",
       "2                     0.027479     Poughkeepsie, NY   \n",
       "3                     0.000000       Pittsburgh, PA   \n",
       "4                     0.000000             York, PA   \n",
       "\n",
       "   PARENT_METRO_REGION_METRO_CODE               LAST_UPDATED  \n",
       "0                           10580  2025-08-01 14:37:32.798 Z  \n",
       "1                           36260  2025-08-01 14:37:32.798 Z  \n",
       "2                           39100  2025-08-01 14:37:32.798 Z  \n",
       "3                           38300  2025-08-01 14:37:32.798 Z  \n",
       "4                           49620  2025-08-01 14:37:32.798 Z  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's Check the first rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c50b7a4-b830-414d-a98c-69be952d732c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PERIOD_BEGIN', 'PERIOD_END', 'PERIOD_DURATION', 'REGION_TYPE',\n",
       "       'REGION_TYPE_ID', 'TABLE_ID', 'IS_SEASONALLY_ADJUSTED', 'REGION',\n",
       "       'CITY', 'STATE', 'STATE_CODE', 'PROPERTY_TYPE', 'PROPERTY_TYPE_ID',\n",
       "       'MEDIAN_SALE_PRICE', 'MEDIAN_SALE_PRICE_MOM', 'MEDIAN_SALE_PRICE_YOY',\n",
       "       'MEDIAN_LIST_PRICE', 'MEDIAN_LIST_PRICE_MOM', 'MEDIAN_LIST_PRICE_YOY',\n",
       "       'MEDIAN_PPSF', 'MEDIAN_PPSF_MOM', 'MEDIAN_PPSF_YOY', 'MEDIAN_LIST_PPSF',\n",
       "       'MEDIAN_LIST_PPSF_MOM', 'MEDIAN_LIST_PPSF_YOY', 'HOMES_SOLD',\n",
       "       'HOMES_SOLD_MOM', 'HOMES_SOLD_YOY', 'PENDING_SALES',\n",
       "       'PENDING_SALES_MOM', 'PENDING_SALES_YOY', 'NEW_LISTINGS',\n",
       "       'NEW_LISTINGS_MOM', 'NEW_LISTINGS_YOY', 'INVENTORY', 'INVENTORY_MOM',\n",
       "       'INVENTORY_YOY', 'MONTHS_OF_SUPPLY', 'MONTHS_OF_SUPPLY_MOM',\n",
       "       'MONTHS_OF_SUPPLY_YOY', 'MEDIAN_DOM', 'MEDIAN_DOM_MOM',\n",
       "       'MEDIAN_DOM_YOY', 'AVG_SALE_TO_LIST', 'AVG_SALE_TO_LIST_MOM',\n",
       "       'AVG_SALE_TO_LIST_YOY', 'SOLD_ABOVE_LIST', 'SOLD_ABOVE_LIST_MOM',\n",
       "       'SOLD_ABOVE_LIST_YOY', 'PRICE_DROPS', 'PRICE_DROPS_MOM',\n",
       "       'PRICE_DROPS_YOY', 'OFF_MARKET_IN_TWO_WEEKS',\n",
       "       'OFF_MARKET_IN_TWO_WEEKS_MOM', 'OFF_MARKET_IN_TWO_WEEKS_YOY',\n",
       "       'PARENT_METRO_REGION', 'PARENT_METRO_REGION_METRO_CODE',\n",
       "       'LAST_UPDATED'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's Check The columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e011f38c-fa4b-46d4-a453-0fc051d04c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cleaned and renamed the 'region' column to 'ZIP_CODE'.\n",
      "   ZIP_CODE\n",
      "0     12210\n",
      "1     84075\n",
      "2     12771\n",
      "3     15047\n",
      "4     17408\n"
     ]
    }
   ],
   "source": [
    "# Replace REGION For Zip Code because Region has strings before the actual zipcode number\n",
    "\n",
    "# Check if 'region' column exists\n",
    "if 'REGION' in df.columns:\n",
    "    # Extract the zip code number by splitting the string\n",
    "    df['ZIP_CODE'] = df['REGION'].str.split(': ').str[1]\n",
    "\n",
    "    # Convert the new column to a numeric type, forcing errors to become NaN (Not a Number)\n",
    "    df['ZIP_CODE'] = pd.to_numeric(df['ZIP_CODE'], errors='coerce')\n",
    "\n",
    "    # Drop the old 'region' column\n",
    "    df.drop(columns=['REGION'], inplace=True)\n",
    "\n",
    "    print(\"Successfully cleaned and renamed the 'region' column to 'ZIP_CODE'.\")\n",
    "    print(df[['ZIP_CODE']].head())\n",
    "else:\n",
    "    print(\"'REGION' column not found. It may have been cleaned already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "567fc83b-59f2-4ce0-b74c-f5dfd76386f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PERIOD_BEGIN                      datetime64[ns]\n",
       "PERIOD_END                        datetime64[ns]\n",
       "PERIOD_DURATION                            int64\n",
       "REGION_TYPE                               object\n",
       "REGION_TYPE_ID                             int64\n",
       "TABLE_ID                                   int64\n",
       "IS_SEASONALLY_ADJUSTED                      bool\n",
       "CITY                                     float64\n",
       "STATE                                     object\n",
       "STATE_CODE                                object\n",
       "PROPERTY_TYPE                             object\n",
       "PROPERTY_TYPE_ID                           int64\n",
       "MEDIAN_SALE_PRICE                        float64\n",
       "MEDIAN_SALE_PRICE_MOM                    float64\n",
       "MEDIAN_SALE_PRICE_YOY                    float64\n",
       "MEDIAN_LIST_PRICE                        float64\n",
       "MEDIAN_LIST_PRICE_MOM                    float64\n",
       "MEDIAN_LIST_PRICE_YOY                    float64\n",
       "MEDIAN_PPSF                              float64\n",
       "MEDIAN_PPSF_MOM                          float64\n",
       "MEDIAN_PPSF_YOY                          float64\n",
       "MEDIAN_LIST_PPSF                         float64\n",
       "MEDIAN_LIST_PPSF_MOM                     float64\n",
       "MEDIAN_LIST_PPSF_YOY                     float64\n",
       "HOMES_SOLD                               float64\n",
       "HOMES_SOLD_MOM                           float64\n",
       "HOMES_SOLD_YOY                           float64\n",
       "PENDING_SALES                            float64\n",
       "PENDING_SALES_MOM                        float64\n",
       "PENDING_SALES_YOY                        float64\n",
       "NEW_LISTINGS                             float64\n",
       "NEW_LISTINGS_MOM                         float64\n",
       "NEW_LISTINGS_YOY                         float64\n",
       "INVENTORY                                float64\n",
       "INVENTORY_MOM                            float64\n",
       "INVENTORY_YOY                            float64\n",
       "MONTHS_OF_SUPPLY                         float64\n",
       "MONTHS_OF_SUPPLY_MOM                     float64\n",
       "MONTHS_OF_SUPPLY_YOY                     float64\n",
       "MEDIAN_DOM                               float64\n",
       "MEDIAN_DOM_MOM                           float64\n",
       "MEDIAN_DOM_YOY                           float64\n",
       "AVG_SALE_TO_LIST                         float64\n",
       "AVG_SALE_TO_LIST_MOM                     float64\n",
       "AVG_SALE_TO_LIST_YOY                     float64\n",
       "SOLD_ABOVE_LIST                          float64\n",
       "SOLD_ABOVE_LIST_MOM                      float64\n",
       "SOLD_ABOVE_LIST_YOY                      float64\n",
       "PRICE_DROPS                              float64\n",
       "PRICE_DROPS_MOM                          float64\n",
       "PRICE_DROPS_YOY                          float64\n",
       "OFF_MARKET_IN_TWO_WEEKS                  float64\n",
       "OFF_MARKET_IN_TWO_WEEKS_MOM              float64\n",
       "OFF_MARKET_IN_TWO_WEEKS_YOY              float64\n",
       "PARENT_METRO_REGION                       object\n",
       "PARENT_METRO_REGION_METRO_CODE             int64\n",
       "LAST_UPDATED                              object\n",
       "ZIP_CODE                                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check column types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7093d3-a93a-427c-8442-a3cd498db14c",
   "metadata": {},
   "source": [
    "## 7. Check uniqueness\n",
    "Now, let's ensure that every row in our dataset represents a unique combination of a zip code and a time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97364498-2f23-4abe-960c-9b773acd7153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Found 6030537 duplicate rows. Removing them now.\n"
     ]
    }
   ],
   "source": [
    "# Define the key columns that should make a row unique\n",
    "key_columns = ['ZIP_CODE', 'PERIOD_BEGIN', 'PERIOD_END','STATE']\n",
    "\n",
    "# Count the number of duplicate rows based on our key\n",
    "num_duplicates = df.duplicated(subset=key_columns).sum()\n",
    "\n",
    "if num_duplicates == 0:\n",
    "    print(\"✅ Success: No duplicate rows found.\")\n",
    "else:\n",
    "    print(f\"⚠️ Found {num_duplicates} duplicate rows. Removing them now.\")\n",
    "    df.drop_duplicates(subset=key_columns, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "545d1637-1828-470f-9d42-8fe1db1e9d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3097102, 58)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96747c-50f0-4834-bb92-4d07a9bc43a7",
   "metadata": {},
   "source": [
    "## 5. Check for Missingness (Missing Values)\n",
    "Finally, let's get a report of missing values in our dataset. This will help us decide how to handle them in the next stage of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e3b4fcc-ea70-4552-9e10-95c6876cc62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Missing Values Report ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_missing</th>\n",
       "      <th>pct_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CITY</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTHS_OF_SUPPLY</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTHS_OF_SUPPLY_MOM</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTHS_OF_SUPPLY_YOY</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRICE_DROPS_YOY</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRICE_DROPS</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRICE_DROPS_MOM</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INVENTORY_YOY</th>\n",
       "      <td>677172</td>\n",
       "      <td>21.864698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_LIST_PPSF_YOY</th>\n",
       "      <td>617396</td>\n",
       "      <td>19.934636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEW_LISTINGS_YOY</th>\n",
       "      <td>615665</td>\n",
       "      <td>19.878745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_LIST_PRICE_YOY</th>\n",
       "      <td>600035</td>\n",
       "      <td>19.374079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PENDING_SALES_YOY</th>\n",
       "      <td>557719</td>\n",
       "      <td>18.007770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG_SALE_TO_LIST_YOY</th>\n",
       "      <td>527850</td>\n",
       "      <td>17.043352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFF_MARKET_IN_TWO_WEEKS_YOY</th>\n",
       "      <td>495129</td>\n",
       "      <td>15.986848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INVENTORY_MOM</th>\n",
       "      <td>486692</td>\n",
       "      <td>15.714432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_DOM_YOY</th>\n",
       "      <td>449413</td>\n",
       "      <td>14.510759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_PPSF_YOY</th>\n",
       "      <td>446323</td>\n",
       "      <td>14.410988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOLD_ABOVE_LIST_YOY</th>\n",
       "      <td>438323</td>\n",
       "      <td>14.152682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_LIST_PPSF_MOM</th>\n",
       "      <td>425634</td>\n",
       "      <td>13.742976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEW_LISTINGS_MOM</th>\n",
       "      <td>421724</td>\n",
       "      <td>13.616729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_SALE_PRICE_YOY</th>\n",
       "      <td>417565</td>\n",
       "      <td>13.482443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOMES_SOLD_YOY</th>\n",
       "      <td>417394</td>\n",
       "      <td>13.476921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_LIST_PRICE_MOM</th>\n",
       "      <td>407637</td>\n",
       "      <td>13.161885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INVENTORY</th>\n",
       "      <td>340910</td>\n",
       "      <td>11.007387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PENDING_SALES_MOM</th>\n",
       "      <td>323835</td>\n",
       "      <td>10.456065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG_SALE_TO_LIST_MOM</th>\n",
       "      <td>279616</td>\n",
       "      <td>9.028311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_LIST_PPSF</th>\n",
       "      <td>266848</td>\n",
       "      <td>8.616055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEW_LISTINGS</th>\n",
       "      <td>262452</td>\n",
       "      <td>8.474115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFF_MARKET_IN_TWO_WEEKS_MOM</th>\n",
       "      <td>253919</td>\n",
       "      <td>8.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_LIST_PRICE</th>\n",
       "      <td>247831</td>\n",
       "      <td>8.002029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_PPSF_MOM</th>\n",
       "      <td>189605</td>\n",
       "      <td>6.122013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_DOM_MOM</th>\n",
       "      <td>188331</td>\n",
       "      <td>6.080878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOLD_ABOVE_LIST_MOM</th>\n",
       "      <td>181180</td>\n",
       "      <td>5.849985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PENDING_SALES</th>\n",
       "      <td>167530</td>\n",
       "      <td>5.409250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_SALE_PRICE_MOM</th>\n",
       "      <td>159301</td>\n",
       "      <td>5.143550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOMES_SOLD_MOM</th>\n",
       "      <td>159132</td>\n",
       "      <td>5.138094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG_SALE_TO_LIST</th>\n",
       "      <td>130728</td>\n",
       "      <td>4.220978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFF_MARKET_IN_TWO_WEEKS</th>\n",
       "      <td>93839</td>\n",
       "      <td>3.029897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_PPSF</th>\n",
       "      <td>36721</td>\n",
       "      <td>1.185657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_DOM</th>\n",
       "      <td>34095</td>\n",
       "      <td>1.100868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOLD_ABOVE_LIST</th>\n",
       "      <td>28242</td>\n",
       "      <td>0.911885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_SALE_PRICE</th>\n",
       "      <td>5285</td>\n",
       "      <td>0.170643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOMES_SOLD</th>\n",
       "      <td>5128</td>\n",
       "      <td>0.165574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             num_missing  pct_missing\n",
       "CITY                             3097102   100.000000\n",
       "MONTHS_OF_SUPPLY                 3097102   100.000000\n",
       "MONTHS_OF_SUPPLY_MOM             3097102   100.000000\n",
       "MONTHS_OF_SUPPLY_YOY             3097102   100.000000\n",
       "PRICE_DROPS_YOY                  3097102   100.000000\n",
       "PRICE_DROPS                      3097102   100.000000\n",
       "PRICE_DROPS_MOM                  3097102   100.000000\n",
       "INVENTORY_YOY                     677172    21.864698\n",
       "MEDIAN_LIST_PPSF_YOY              617396    19.934636\n",
       "NEW_LISTINGS_YOY                  615665    19.878745\n",
       "MEDIAN_LIST_PRICE_YOY             600035    19.374079\n",
       "PENDING_SALES_YOY                 557719    18.007770\n",
       "AVG_SALE_TO_LIST_YOY              527850    17.043352\n",
       "OFF_MARKET_IN_TWO_WEEKS_YOY       495129    15.986848\n",
       "INVENTORY_MOM                     486692    15.714432\n",
       "MEDIAN_DOM_YOY                    449413    14.510759\n",
       "MEDIAN_PPSF_YOY                   446323    14.410988\n",
       "SOLD_ABOVE_LIST_YOY               438323    14.152682\n",
       "MEDIAN_LIST_PPSF_MOM              425634    13.742976\n",
       "NEW_LISTINGS_MOM                  421724    13.616729\n",
       "MEDIAN_SALE_PRICE_YOY             417565    13.482443\n",
       "HOMES_SOLD_YOY                    417394    13.476921\n",
       "MEDIAN_LIST_PRICE_MOM             407637    13.161885\n",
       "INVENTORY                         340910    11.007387\n",
       "PENDING_SALES_MOM                 323835    10.456065\n",
       "AVG_SALE_TO_LIST_MOM              279616     9.028311\n",
       "MEDIAN_LIST_PPSF                  266848     8.616055\n",
       "NEW_LISTINGS                      262452     8.474115\n",
       "OFF_MARKET_IN_TWO_WEEKS_MOM       253919     8.198600\n",
       "MEDIAN_LIST_PRICE                 247831     8.002029\n",
       "MEDIAN_PPSF_MOM                   189605     6.122013\n",
       "MEDIAN_DOM_MOM                    188331     6.080878\n",
       "SOLD_ABOVE_LIST_MOM               181180     5.849985\n",
       "PENDING_SALES                     167530     5.409250\n",
       "MEDIAN_SALE_PRICE_MOM             159301     5.143550\n",
       "HOMES_SOLD_MOM                    159132     5.138094\n",
       "AVG_SALE_TO_LIST                  130728     4.220978\n",
       "OFF_MARKET_IN_TWO_WEEKS            93839     3.029897\n",
       "MEDIAN_PPSF                        36721     1.185657\n",
       "MEDIAN_DOM                         34095     1.100868\n",
       "SOLD_ABOVE_LIST                    28242     0.911885\n",
       "MEDIAN_SALE_PRICE                   5285     0.170643\n",
       "HOMES_SOLD                          5128     0.165574"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the number of missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "# Create a report\n",
    "missing_report = pd.DataFrame({\n",
    "    'num_missing': missing_values,\n",
    "    'pct_missing': missing_percentage\n",
    "})\n",
    "\n",
    "# Show columns with at least one missing value, sorted by percentage\n",
    "print(\"--- Missing Values Report ---\")\n",
    "display(missing_report[missing_report['num_missing'] > 0].sort_values(by='pct_missing', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5e58b58-3bd9-4fa9-bbbc-e6d309ba1847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (3097102, 58)\n"
     ]
    }
   ],
   "source": [
    "# --- Create a copy to work on ---\n",
    "df_cleaned = df.copy()\n",
    "print(f\"Original DataFrame shape: {df_cleaned.shape}\")\n",
    "\n",
    "# Ensure data is sorted for time-series operations\n",
    "df_cleaned.sort_values(by=['ZIP_CODE', 'PERIOD_BEGIN'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26a784-53dc-45b9-8d77-59a674ef9a59",
   "metadata": {},
   "source": [
    "### 5.1. Drop 100% Missing Columns\n",
    "It does not give any value to our case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "083f9f2f-06c6-4d09-852a-ec7befdf6fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Dropped 7 unusable columns.\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Drop Unusable Columns (100% Missing) ===\n",
    "columns_to_drop = [\n",
    "    'CITY', 'MONTHS_OF_SUPPLY', 'MONTHS_OF_SUPPLY_MOM', 'MONTHS_OF_SUPPLY_YOY',\n",
    "    'PRICE_DROPS', 'PRICE_DROPS_MOM', 'PRICE_DROPS_YOY'\n",
    "]\n",
    "df_cleaned.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "print(f\"\\nStep 1: Dropped {len(columns_to_drop)} unusable columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671cab20-5d7e-4b30-9d71-67c6b66bf0a0",
   "metadata": {},
   "source": [
    "### 5.2. Reconstruct Derived Metrics (YOY & MOM)\n",
    "For all Year-over-Year (YOY) and Month-over-Month (MOM) columns, we will first attempt to recalculate them from their raw source columns. This is the most accurate way to fill them in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bf16dd6-88b7-4138-874d-c0b0e596d849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Reconstructed all YOY and MOM metrics.\n"
     ]
    }
   ],
   "source": [
    "# === Step 2: Reconstruct All Derived Metrics (YOY & MOM) ===\n",
    "# Dictionary of {derived_metric: raw_metric}\n",
    "reconstruction_map = {\n",
    "    # Year-over-Year (shift 4 quarters)\n",
    "    'MEDIAN_SALE_PRICE_YOY': 'MEDIAN_SALE_PRICE',\n",
    "    'HOMES_SOLD_YOY': 'HOMES_SOLD',\n",
    "    'PENDING_SALES_YOY': 'PENDING_SALES',\n",
    "    'NEW_LISTINGS_YOY': 'NEW_LISTINGS',\n",
    "    'INVENTORY_YOY': 'INVENTORY',\n",
    "    'MEDIAN_DOM_YOY': 'MEDIAN_DOM',\n",
    "    'MEDIAN_LIST_PRICE_YOY': 'MEDIAN_LIST_PRICE',\n",
    "    'MEDIAN_PPSF_YOY': 'MEDIAN_PPSF',\n",
    "    'MEDIAN_LIST_PPSF_YOY': 'MEDIAN_LIST_PPSF',\n",
    "    'AVG_SALE_TO_LIST_YOY': 'AVG_SALE_TO_LIST',\n",
    "    'SOLD_ABOVE_LIST_YOY': 'SOLD_ABOVE_LIST',\n",
    "    # Month-over-Month (shift 1 quarter)\n",
    "    'HOMES_SOLD_MOM': 'HOMES_SOLD',\n",
    "    'PENDING_SALES_MOM': 'PENDING_SALES',\n",
    "    'NEW_LISTINGS_MOM': 'NEW_LISTINGS',\n",
    "    'INVENTORY_MOM': 'INVENTORY',\n",
    "    'MEDIAN_DOM_MOM': 'MEDIAN_DOM',\n",
    "    'MEDIAN_LIST_PRICE_MOM': 'MEDIAN_LIST_PRICE',\n",
    "    'MEDIAN_SALE_PRICE_MOM': 'MEDIAN_SALE_PRICE'\n",
    "}\n",
    "\n",
    "for derived_col, raw_col in reconstruction_map.items():\n",
    "    if raw_col in df_cleaned.columns and derived_col in df_cleaned.columns:\n",
    "        shift_period = 4 if '_YOY' in derived_col else 1\n",
    "        \n",
    "        # Get the value from the previous period\n",
    "        last_period_val = df_cleaned.groupby('ZIP_CODE')[raw_col].shift(shift_period)\n",
    "        \n",
    "        # Recalculate the percentage change\n",
    "        recomputed_val = ((df_cleaned[raw_col] - last_period_val) / last_period_val) * 100\n",
    "        \n",
    "        # Fill missing values in the derived column with our calculation\n",
    "        df_cleaned[derived_col] = df_cleaned[derived_col].fillna(recomputed_val)\n",
    "\n",
    "print(\"Step 2: Reconstructed all YOY metrics.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d83a9-3076-4481-945b-6a1901159ab0",
   "metadata": {},
   "source": [
    "### 5.3 Drop MoM data:\n",
    "A _MOM column would imply a comparison to the previous month, but that data doesn't exist. Instead, the comparison is to the previous quarter. This makes the _MOM label misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "265e4031-db6c-4862-9ace-c2a54e1c321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully dropped 12 misleading '_MOM' columns.\n",
      "\n",
      "Remaining columns:\n",
      "Index(['PERIOD_BEGIN', 'PERIOD_END', 'PERIOD_DURATION', 'REGION_TYPE',\n",
      "       'REGION_TYPE_ID', 'TABLE_ID', 'IS_SEASONALLY_ADJUSTED', 'STATE',\n",
      "       'STATE_CODE', 'PROPERTY_TYPE', 'PROPERTY_TYPE_ID', 'MEDIAN_SALE_PRICE',\n",
      "       'MEDIAN_SALE_PRICE_YOY', 'MEDIAN_LIST_PRICE', 'MEDIAN_LIST_PRICE_YOY',\n",
      "       'MEDIAN_PPSF', 'MEDIAN_PPSF_YOY', 'MEDIAN_LIST_PPSF',\n",
      "       'MEDIAN_LIST_PPSF_YOY', 'HOMES_SOLD', 'HOMES_SOLD_YOY', 'PENDING_SALES',\n",
      "       'PENDING_SALES_YOY', 'NEW_LISTINGS', 'NEW_LISTINGS_YOY', 'INVENTORY',\n",
      "       'INVENTORY_YOY', 'MEDIAN_DOM', 'MEDIAN_DOM_YOY', 'AVG_SALE_TO_LIST',\n",
      "       'AVG_SALE_TO_LIST_YOY', 'SOLD_ABOVE_LIST', 'SOLD_ABOVE_LIST_YOY',\n",
      "       'OFF_MARKET_IN_TWO_WEEKS', 'OFF_MARKET_IN_TWO_WEEKS_YOY',\n",
      "       'PARENT_METRO_REGION', 'PARENT_METRO_REGION_METRO_CODE', 'LAST_UPDATED',\n",
      "       'ZIP_CODE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Find all columns that contain '_MOM' in their name\n",
    "mom_columns = [col for col in df_cleaned.columns if '_MOM' in col]\n",
    "\n",
    "# Drop these columns from the dataframe\n",
    "df_cleaned.drop(columns=mom_columns, inplace=True)\n",
    "\n",
    "print(f\"Successfully dropped {len(mom_columns)} misleading '_MOM' columns.\")print(df_cleaned.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cbf56-d0a9-4b25-8aa2-9ba3e336fdf9",
   "metadata": {},
   "source": [
    "### 5.4. Impute Secondary Metrics: \n",
    "For important raw metrics with moderate (5-15%) missingness like INVENTORY or NEW_LISTINGS, we'll use a time-series forward-fill (ffill). This assumes that if data for a period is missing, market conditions were similar to the last known period for that same zip code—a very reasonable approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "240f2948-7451-4553-b942-ca702c90e520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Imputed secondary metrics using forward-fill.\n"
     ]
    }
   ],
   "source": [
    "# === Step 4: Impute Secondary Raw Metrics (5-15% Missing) ===\n",
    "secondary_cols_to_impute = [\n",
    "    'PENDING_SALES', 'MEDIAN_LIST_PRICE', 'NEW_LISTINGS',\n",
    "    'MEDIAN_LIST_PPSF', 'INVENTORY'\n",
    "]\n",
    "# Use forward-fill within each zip code group\n",
    "df_cleaned[secondary_cols_to_impute] = df_cleaned.groupby('ZIP_CODE')[secondary_cols_to_impute].ffill()\n",
    "print(\"Step 4: Imputed secondary metrics using forward-fill.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb31e2bf-8df4-4ec8-959c-ffd57136291b",
   "metadata": {},
   "source": [
    "### 5.5. Final Cleanup:\n",
    "For the most essential raw metrics with very low (<5%) missingness (MEDIAN_SALE_PRICE, MEDIAN_DOM), we will drop the few remaining rows. This ensures our core analysis is based purely on observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52b47e25-bd33-42c6-a651-cf44d02ff5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Dropped remaining rows with missing essential data.\n",
      "\n",
      "Cleaned DataFrame shape: (3062850, 39)\n",
      "\n",
      "--- Remaining Missing Values Report ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OFF_MARKET_IN_TWO_WEEKS_YOY    482667\n",
       "INVENTORY_YOY                  401770\n",
       "MEDIAN_LIST_PPSF_YOY           345410\n",
       "NEW_LISTINGS_YOY               334579\n",
       "MEDIAN_LIST_PRICE_YOY          325111\n",
       "SOLD_ABOVE_LIST_YOY            227157\n",
       "PENDING_SALES_YOY              223479\n",
       "AVG_SALE_TO_LIST_YOY           174711\n",
       "AVG_SALE_TO_LIST               124266\n",
       "OFF_MARKET_IN_TWO_WEEKS         91171\n",
       "MEDIAN_PPSF_YOY                 78842\n",
       "PENDING_SALES                   78359\n",
       "INVENTORY                       72571\n",
       "MEDIAN_DOM_YOY                  50482\n",
       "MEDIAN_SALE_PRICE_YOY           43449\n",
       "HOMES_SOLD_YOY                  43421\n",
       "MEDIAN_PPSF                     30600\n",
       "SOLD_ABOVE_LIST                 28136\n",
       "NEW_LISTINGS                    25304\n",
       "MEDIAN_LIST_PPSF                17439\n",
       "MEDIAN_LIST_PRICE               14055\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Step 5: Final Cleanup of Essential Metrics (<5% Missing) ===\n",
    "# These are the absolute minimum columns needed for a valid record\n",
    "essential_columns = [\n",
    "    'MEDIAN_SALE_PRICE', 'HOMES_SOLD', 'MEDIAN_DOM'\n",
    "]\n",
    "df_cleaned.dropna(subset=essential_columns, inplace=True)\n",
    "print(\"Step 5: Dropped remaining rows with missing essential data.\")\n",
    "\n",
    "\n",
    "# === Final Report ===\n",
    "print(f\"\\nCleaned DataFrame shape: {df_cleaned.shape}\")\n",
    "print(\"\\n--- Remaining Missing Values Report ---\")\n",
    "missing_report = df_cleaned.isnull().sum()\n",
    "display(missing_report[missing_report > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab45d2-c4d8-41e5-a166-4299d0f15b03",
   "metadata": {},
   "source": [
    "### 5.6. Saved the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3996fa95-a6dc-42c7-9ae2-5876709c4b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the path for the cleaned data\n",
    "cleaned_data_path = 'data/cleaned_data/redfin_cleaned.csv'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(cleaned_data_path), exist_ok=True)\n",
    "\n",
    "# Save the cleaned dataframe to a CSV file\n",
    "df_cleaned.to_csv(cleaned_data_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset successfully saved to: {cleaned_data_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
