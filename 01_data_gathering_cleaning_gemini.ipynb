{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5444a705-878a-49a9-99e7-9e6b6bb68cc8",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries\n",
    "First, let's import the libraries we'll need for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96c21836-2788-43b1-b9a4-54e823ad6326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import gzip\n",
    "import shutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a7ff0-f920-4cad-a9aa-b2e85c04281e",
   "metadata": {},
   "source": [
    "## 2. Define File Paths and URL\n",
    "Next, we'll define the URL for the data and the names of the files we'll be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eca332bb-6577-4412-a54f-f9b3fd7d4c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL for the Redfin data\n",
    "url = \"https://redfin-public-data.s3.us-west-2.amazonaws.com/redfin_market_tracker/zip_code_market_tracker.tsv000.gz\"\n",
    "\n",
    "# File names for the downloaded and processed data\n",
    "gzipped_file = 'data/raw/zip_code_market_tracker.tsv000.gz'\n",
    "unzipped_file = 'data/raw/zip_code_market_tracker.tsv'\n",
    "# CHANGED: The final raw file is now saved in the data/raw/ directory\n",
    "final_csv_file = 'data/raw/redfin_raw_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0752a4bb-b33d-4a76-bda4-fcd361183e70",
   "metadata": {},
   "source": [
    "## 3. Download the Data\n",
    "This function will download the file from the URL. It includes a basic check to see if the download was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "36c1b155-4d29-4912-b75f-e9d4a1e031ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url, filename):\n",
    "    \"\"\"Downloads a file from a given URL.\"\"\"\n",
    "    try:\n",
    "        with requests.get(url, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(filename, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "        print(f\"Successfully downloaded {filename}\")\n",
    "        return True\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return False\n",
    "\n",
    "def decompress_gzip(gzipped_file, unzipped_file):\n",
    "    \"\"\"Decompresses a gzipped file.\"\"\"\n",
    "    try:\n",
    "        with gzip.open(gzipped_file, 'rb') as f_in:\n",
    "            with open(unzipped_file, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        print(f\"Successfully decompressed {gzipped_file} to {unzipped_file}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error decompressing file: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa550836-095b-45c2-8232-fa69db692237",
   "metadata": {},
   "source": [
    "## 4. Load and Process the Data\n",
    "This is the core of the script. It orchestrates the downloading, loading, and merging of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae18e61a-7a35-4b94-ab97-97d2ed5b8b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded data/raw/zip_code_market_tracker.tsv000.gz\n",
      "Successfully decompressed data/raw/zip_code_market_tracker.tsv000.gz to data/raw/zip_code_market_tracker.tsv\n",
      "Successfully loaded new data.\n",
      "Existing dataset found. Merging with new data.\n",
      "Successfully merged and removed duplicates.\n",
      "Raw dataset saved to data/raw/redfin_raw_data.csv\n",
      "Cleaned up temporary files.\n"
     ]
    }
   ],
   "source": [
    "def update_dataset(url, gzipped_file, unzipped_file, final_csv_file):\n",
    "    \"\"\"\n",
    "    Downloads, processes, and updates the Redfin dataset.\n",
    "    \"\"\"\n",
    "    # ADDED: Create the data/raw directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(gzipped_file), exist_ok=True)\n",
    "    \n",
    "    if not download_file(url, gzipped_file):\n",
    "        return\n",
    "\n",
    "    if not decompress_gzip(gzipped_file, unzipped_file):\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        new_data = pd.read_csv(unzipped_file, sep='\\t')\n",
    "        print(\"Successfully loaded new data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        return\n",
    "\n",
    "    if os.path.exists(final_csv_file):\n",
    "        print(\"Existing dataset found. Merging with new data.\")\n",
    "        old_data = pd.read_csv(final_csv_file)\n",
    "        combined_data = pd.concat([old_data, new_data], ignore_index=True)\n",
    "        # CHANGED: Column names are lowercase in the raw file\n",
    "        combined_data.drop_duplicates(subset=['REGION', 'PERIOD_BEGIN', 'PERIOD_END'], keep='last', inplace=True)\n",
    "        print(\"Successfully merged and removed duplicates.\")\n",
    "    else:\n",
    "        print(\"No existing dataset found. Creating a new one.\")\n",
    "        combined_data = new_data\n",
    "\n",
    "    combined_data.to_csv(final_csv_file, index=False)\n",
    "    print(f\"Raw dataset saved to {final_csv_file}\")\n",
    "\n",
    "    # Clean up temporary downloaded files\n",
    "    os.remove(gzipped_file)\n",
    "    os.remove(unzipped_file)\n",
    "    print(\"Cleaned up temporary files.\")\n",
    "\n",
    "# --- Step 4: Run the Process ---\n",
    "update_dataset(url, gzipped_file, unzipped_file, final_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc772cfe-7e27-48d5-90f9-a2cd627f4b30",
   "metadata": {},
   "source": [
    "## 6. Load the Dataset\n",
    "Inspect the dataset it's shape, types,typos, etc.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a07468a-a359-425e-86f6-a14a75d84401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "The dataset contains 3,097,102 rows and 58 columns.\n",
      "\n",
      "Most recent data is from: 2025-05-01\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv('data/raw/redfin_raw_data.csv')\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "    print(f\"The dataset contains {df.shape[0]:,} rows and {df.shape[1]} columns.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'redfin_zipcode_data.csv' not found. Please run the data download script first.\")\n",
    "\n",
    "# --- Data Preparation ---\n",
    "\n",
    "# Convert date columns to datetime objects\n",
    "df['PERIOD_BEGIN'] = pd.to_datetime(df['PERIOD_BEGIN'])\n",
    "df['PERIOD_END'] = pd.to_datetime(df['PERIOD_END'])\n",
    "\n",
    "# For this analysis, we want the most recent data for each zip code.\n",
    "# Let's find the latest period available in the dataset.\n",
    "latest_date = df['PERIOD_BEGIN'].max()\n",
    "print(f\"\\nMost recent data is from: {latest_date.strftime('%Y-%m-%d')}\")\n",
    "\n",
    "# Create a dataframe with only the most recent data for each zip code\n",
    "latest_df = df[df['PERIOD_BEGIN'] == latest_date].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a2b44ba-8496-479c-bcf7-7554a04923d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PERIOD_BEGIN</th>\n",
       "      <th>PERIOD_END</th>\n",
       "      <th>PERIOD_DURATION</th>\n",
       "      <th>REGION_TYPE</th>\n",
       "      <th>REGION_TYPE_ID</th>\n",
       "      <th>TABLE_ID</th>\n",
       "      <th>IS_SEASONALLY_ADJUSTED</th>\n",
       "      <th>REGION</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATE</th>\n",
       "      <th>...</th>\n",
       "      <th>SOLD_ABOVE_LIST_YOY</th>\n",
       "      <th>PRICE_DROPS</th>\n",
       "      <th>PRICE_DROPS_MOM</th>\n",
       "      <th>PRICE_DROPS_YOY</th>\n",
       "      <th>OFF_MARKET_IN_TWO_WEEKS</th>\n",
       "      <th>OFF_MARKET_IN_TWO_WEEKS_MOM</th>\n",
       "      <th>OFF_MARKET_IN_TWO_WEEKS_YOY</th>\n",
       "      <th>PARENT_METRO_REGION</th>\n",
       "      <th>PARENT_METRO_REGION_METRO_CODE</th>\n",
       "      <th>LAST_UPDATED</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014-07-01</td>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>90</td>\n",
       "      <td>zip code</td>\n",
       "      <td>2</td>\n",
       "      <td>15721</td>\n",
       "      <td>False</td>\n",
       "      <td>Zip Code: 37145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tennessee</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>34980</td>\n",
       "      <td>2025-08-01 14:37:32.798 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-10-01</td>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>90</td>\n",
       "      <td>zip code</td>\n",
       "      <td>2</td>\n",
       "      <td>17323</td>\n",
       "      <td>False</td>\n",
       "      <td>Zip Code: 40903</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kentucky</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>London, KY</td>\n",
       "      <td>30940</td>\n",
       "      <td>2025-08-01 14:37:32.798 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-08-01</td>\n",
       "      <td>2021-10-31</td>\n",
       "      <td>90</td>\n",
       "      <td>zip code</td>\n",
       "      <td>2</td>\n",
       "      <td>30991</td>\n",
       "      <td>False</td>\n",
       "      <td>Zip Code: 72386</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Arkansas</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jonesboro, AR</td>\n",
       "      <td>27860</td>\n",
       "      <td>2025-08-01 14:37:32.798 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-06-01</td>\n",
       "      <td>2017-08-31</td>\n",
       "      <td>90</td>\n",
       "      <td>zip code</td>\n",
       "      <td>2</td>\n",
       "      <td>19279</td>\n",
       "      <td>False</td>\n",
       "      <td>Zip Code: 45764</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.088889</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>Athens, OH</td>\n",
       "      <td>11900</td>\n",
       "      <td>2025-08-01 14:37:32.798 Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-09-01</td>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>90</td>\n",
       "      <td>zip code</td>\n",
       "      <td>2</td>\n",
       "      <td>33520</td>\n",
       "      <td>False</td>\n",
       "      <td>Zip Code: 77577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Texas</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>Houston, TX</td>\n",
       "      <td>26420</td>\n",
       "      <td>2025-08-01 14:37:32.798 Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  PERIOD_BEGIN PERIOD_END  PERIOD_DURATION REGION_TYPE  REGION_TYPE_ID  \\\n",
       "0   2014-07-01 2014-09-30               90    zip code               2   \n",
       "1   2021-10-01 2021-12-31               90    zip code               2   \n",
       "2   2021-08-01 2021-10-31               90    zip code               2   \n",
       "3   2017-06-01 2017-08-31               90    zip code               2   \n",
       "4   2020-09-01 2020-11-30               90    zip code               2   \n",
       "\n",
       "   TABLE_ID  IS_SEASONALLY_ADJUSTED           REGION  CITY      STATE  ...  \\\n",
       "0     15721                   False  Zip Code: 37145   NaN  Tennessee  ...   \n",
       "1     17323                   False  Zip Code: 40903   NaN   Kentucky  ...   \n",
       "2     30991                   False  Zip Code: 72386   NaN   Arkansas  ...   \n",
       "3     19279                   False  Zip Code: 45764   NaN       Ohio  ...   \n",
       "4     33520                   False  Zip Code: 77577   NaN      Texas  ...   \n",
       "\n",
       "  SOLD_ABOVE_LIST_YOY PRICE_DROPS  PRICE_DROPS_MOM  PRICE_DROPS_YOY  \\\n",
       "0            0.000000         NaN              NaN              NaN   \n",
       "1                 NaN         NaN              NaN              NaN   \n",
       "2                 NaN         NaN              NaN              NaN   \n",
       "3            0.142857         NaN              NaN              NaN   \n",
       "4           -0.333333         NaN              NaN              NaN   \n",
       "\n",
       "   OFF_MARKET_IN_TWO_WEEKS  OFF_MARKET_IN_TWO_WEEKS_MOM  \\\n",
       "0                      0.0                     0.000000   \n",
       "1                      NaN                          NaN   \n",
       "2                      0.0                     0.000000   \n",
       "3                      0.2                     0.088889   \n",
       "4                      0.4                     0.000000   \n",
       "\n",
       "   OFF_MARKET_IN_TWO_WEEKS_YOY  PARENT_METRO_REGION  \\\n",
       "0                     0.000000        Nashville, TN   \n",
       "1                          NaN           London, KY   \n",
       "2                          NaN        Jonesboro, AR   \n",
       "3                     0.200000           Athens, OH   \n",
       "4                     0.066667          Houston, TX   \n",
       "\n",
       "   PARENT_METRO_REGION_METRO_CODE               LAST_UPDATED  \n",
       "0                           34980  2025-08-01 14:37:32.798 Z  \n",
       "1                           30940  2025-08-01 14:37:32.798 Z  \n",
       "2                           27860  2025-08-01 14:37:32.798 Z  \n",
       "3                           11900  2025-08-01 14:37:32.798 Z  \n",
       "4                           26420  2025-08-01 14:37:32.798 Z  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's Check the first rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c50b7a4-b830-414d-a98c-69be952d732c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['PERIOD_BEGIN', 'PERIOD_END', 'PERIOD_DURATION', 'REGION_TYPE',\n",
       "       'REGION_TYPE_ID', 'TABLE_ID', 'IS_SEASONALLY_ADJUSTED', 'REGION',\n",
       "       'CITY', 'STATE', 'STATE_CODE', 'PROPERTY_TYPE', 'PROPERTY_TYPE_ID',\n",
       "       'MEDIAN_SALE_PRICE', 'MEDIAN_SALE_PRICE_MOM', 'MEDIAN_SALE_PRICE_YOY',\n",
       "       'MEDIAN_LIST_PRICE', 'MEDIAN_LIST_PRICE_MOM', 'MEDIAN_LIST_PRICE_YOY',\n",
       "       'MEDIAN_PPSF', 'MEDIAN_PPSF_MOM', 'MEDIAN_PPSF_YOY', 'MEDIAN_LIST_PPSF',\n",
       "       'MEDIAN_LIST_PPSF_MOM', 'MEDIAN_LIST_PPSF_YOY', 'HOMES_SOLD',\n",
       "       'HOMES_SOLD_MOM', 'HOMES_SOLD_YOY', 'PENDING_SALES',\n",
       "       'PENDING_SALES_MOM', 'PENDING_SALES_YOY', 'NEW_LISTINGS',\n",
       "       'NEW_LISTINGS_MOM', 'NEW_LISTINGS_YOY', 'INVENTORY', 'INVENTORY_MOM',\n",
       "       'INVENTORY_YOY', 'MONTHS_OF_SUPPLY', 'MONTHS_OF_SUPPLY_MOM',\n",
       "       'MONTHS_OF_SUPPLY_YOY', 'MEDIAN_DOM', 'MEDIAN_DOM_MOM',\n",
       "       'MEDIAN_DOM_YOY', 'AVG_SALE_TO_LIST', 'AVG_SALE_TO_LIST_MOM',\n",
       "       'AVG_SALE_TO_LIST_YOY', 'SOLD_ABOVE_LIST', 'SOLD_ABOVE_LIST_MOM',\n",
       "       'SOLD_ABOVE_LIST_YOY', 'PRICE_DROPS', 'PRICE_DROPS_MOM',\n",
       "       'PRICE_DROPS_YOY', 'OFF_MARKET_IN_TWO_WEEKS',\n",
       "       'OFF_MARKET_IN_TWO_WEEKS_MOM', 'OFF_MARKET_IN_TWO_WEEKS_YOY',\n",
       "       'PARENT_METRO_REGION', 'PARENT_METRO_REGION_METRO_CODE',\n",
       "       'LAST_UPDATED'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's Check The columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e011f38c-fa4b-46d4-a453-0fc051d04c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully cleaned and renamed the 'region' column to 'ZIP_CODE'.\n",
      "   ZIP_CODE\n",
      "0     37145\n",
      "1     40903\n",
      "2     72386\n",
      "3     45764\n",
      "4     77577\n"
     ]
    }
   ],
   "source": [
    "# Replace REGION For Zip Code because Region has strings before the actual zipcode number\n",
    "\n",
    "# Check if 'region' column exists\n",
    "if 'REGION' in df.columns:\n",
    "    # Extract the zip code number by splitting the string\n",
    "    df['ZIP_CODE'] = df['REGION'].str.split(': ').str[1]\n",
    "\n",
    "    # Convert the new column to a numeric type, forcing errors to become NaN (Not a Number)\n",
    "    df['ZIP_CODE'] = pd.to_numeric(df['ZIP_CODE'], errors='coerce')\n",
    "\n",
    "    # Drop the old 'region' column\n",
    "    df.drop(columns=['REGION'], inplace=True)\n",
    "\n",
    "    print(\"Successfully cleaned and renamed the 'region' column to 'ZIP_CODE'.\")\n",
    "    print(df[['ZIP_CODE']].head())\n",
    "else:\n",
    "    print(\"'REGION' column not found. It may have been cleaned already.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "567fc83b-59f2-4ce0-b74c-f5dfd76386f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PERIOD_BEGIN                      datetime64[ns]\n",
       "PERIOD_END                        datetime64[ns]\n",
       "PERIOD_DURATION                            int64\n",
       "REGION_TYPE                               object\n",
       "REGION_TYPE_ID                             int64\n",
       "TABLE_ID                                   int64\n",
       "IS_SEASONALLY_ADJUSTED                      bool\n",
       "CITY                                     float64\n",
       "STATE                                     object\n",
       "STATE_CODE                                object\n",
       "PROPERTY_TYPE                             object\n",
       "PROPERTY_TYPE_ID                           int64\n",
       "MEDIAN_SALE_PRICE                        float64\n",
       "MEDIAN_SALE_PRICE_MOM                    float64\n",
       "MEDIAN_SALE_PRICE_YOY                    float64\n",
       "MEDIAN_LIST_PRICE                        float64\n",
       "MEDIAN_LIST_PRICE_MOM                    float64\n",
       "MEDIAN_LIST_PRICE_YOY                    float64\n",
       "MEDIAN_PPSF                              float64\n",
       "MEDIAN_PPSF_MOM                          float64\n",
       "MEDIAN_PPSF_YOY                          float64\n",
       "MEDIAN_LIST_PPSF                         float64\n",
       "MEDIAN_LIST_PPSF_MOM                     float64\n",
       "MEDIAN_LIST_PPSF_YOY                     float64\n",
       "HOMES_SOLD                               float64\n",
       "HOMES_SOLD_MOM                           float64\n",
       "HOMES_SOLD_YOY                           float64\n",
       "PENDING_SALES                            float64\n",
       "PENDING_SALES_MOM                        float64\n",
       "PENDING_SALES_YOY                        float64\n",
       "NEW_LISTINGS                             float64\n",
       "NEW_LISTINGS_MOM                         float64\n",
       "NEW_LISTINGS_YOY                         float64\n",
       "INVENTORY                                float64\n",
       "INVENTORY_MOM                            float64\n",
       "INVENTORY_YOY                            float64\n",
       "MONTHS_OF_SUPPLY                         float64\n",
       "MONTHS_OF_SUPPLY_MOM                     float64\n",
       "MONTHS_OF_SUPPLY_YOY                     float64\n",
       "MEDIAN_DOM                               float64\n",
       "MEDIAN_DOM_MOM                           float64\n",
       "MEDIAN_DOM_YOY                           float64\n",
       "AVG_SALE_TO_LIST                         float64\n",
       "AVG_SALE_TO_LIST_MOM                     float64\n",
       "AVG_SALE_TO_LIST_YOY                     float64\n",
       "SOLD_ABOVE_LIST                          float64\n",
       "SOLD_ABOVE_LIST_MOM                      float64\n",
       "SOLD_ABOVE_LIST_YOY                      float64\n",
       "PRICE_DROPS                              float64\n",
       "PRICE_DROPS_MOM                          float64\n",
       "PRICE_DROPS_YOY                          float64\n",
       "OFF_MARKET_IN_TWO_WEEKS                  float64\n",
       "OFF_MARKET_IN_TWO_WEEKS_MOM              float64\n",
       "OFF_MARKET_IN_TWO_WEEKS_YOY              float64\n",
       "PARENT_METRO_REGION                       object\n",
       "PARENT_METRO_REGION_METRO_CODE             int64\n",
       "LAST_UPDATED                              object\n",
       "ZIP_CODE                                   int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check column types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7093d3-a93a-427c-8442-a3cd498db14c",
   "metadata": {},
   "source": [
    "## 7. Check uniqueness\n",
    "Now, let's ensure that every row in our dataset represents a unique combination of a zip code and a time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97364498-2f23-4abe-960c-9b773acd7153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success: No duplicate rows found.\n"
     ]
    }
   ],
   "source": [
    "# Define the key columns that should make a row unique\n",
    "key_columns = ['ZIP_CODE', 'PERIOD_BEGIN', 'PERIOD_END','STATE']\n",
    "\n",
    "# Count the number of duplicate rows based on our key\n",
    "num_duplicates = df.duplicated(subset=key_columns).sum()\n",
    "\n",
    "if num_duplicates == 0:\n",
    "    print(\"✅ Success: No duplicate rows found.\")\n",
    "else:\n",
    "    print(f\"⚠️ Found {num_duplicates} duplicate rows. Removing them now.\")\n",
    "    df.drop_duplicates(subset=key_columns, keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "545d1637-1828-470f-9d42-8fe1db1e9d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3097102, 58)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e96747c-50f0-4834-bb92-4d07a9bc43a7",
   "metadata": {},
   "source": [
    "## 5. Check for Missingness (Missing Values)\n",
    "Finally, let's get a report of missing values in our dataset. This will help us decide how to handle them in the next stage of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e3b4fcc-ea70-4552-9e10-95c6876cc62f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Missing Values Report ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num_missing</th>\n",
       "      <th>pct_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>CITY</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTHS_OF_SUPPLY</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTHS_OF_SUPPLY_MOM</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTHS_OF_SUPPLY_YOY</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRICE_DROPS_YOY</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRICE_DROPS</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PRICE_DROPS_MOM</th>\n",
       "      <td>3097102</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INVENTORY_YOY</th>\n",
       "      <td>676963</td>\n",
       "      <td>21.857950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_LIST_PPSF_YOY</th>\n",
       "      <td>617045</td>\n",
       "      <td>19.923302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEW_LISTINGS_YOY</th>\n",
       "      <td>615224</td>\n",
       "      <td>19.864506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_LIST_PRICE_YOY</th>\n",
       "      <td>599623</td>\n",
       "      <td>19.360777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PENDING_SALES_YOY</th>\n",
       "      <td>557389</td>\n",
       "      <td>17.997115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG_SALE_TO_LIST_YOY</th>\n",
       "      <td>527954</td>\n",
       "      <td>17.046710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFF_MARKET_IN_TWO_WEEKS_YOY</th>\n",
       "      <td>494734</td>\n",
       "      <td>15.974094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INVENTORY_MOM</th>\n",
       "      <td>486490</td>\n",
       "      <td>15.707910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_DOM_YOY</th>\n",
       "      <td>449354</td>\n",
       "      <td>14.508854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_PPSF_YOY</th>\n",
       "      <td>446351</td>\n",
       "      <td>14.411892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOLD_ABOVE_LIST_YOY</th>\n",
       "      <td>438163</td>\n",
       "      <td>14.147516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_LIST_PPSF_MOM</th>\n",
       "      <td>425199</td>\n",
       "      <td>13.728931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEW_LISTINGS_MOM</th>\n",
       "      <td>421046</td>\n",
       "      <td>13.594838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_SALE_PRICE_YOY</th>\n",
       "      <td>417409</td>\n",
       "      <td>13.477406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOMES_SOLD_YOY</th>\n",
       "      <td>417226</td>\n",
       "      <td>13.471497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_LIST_PRICE_MOM</th>\n",
       "      <td>406937</td>\n",
       "      <td>13.139283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INVENTORY</th>\n",
       "      <td>340390</td>\n",
       "      <td>10.990597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PENDING_SALES_MOM</th>\n",
       "      <td>323652</td>\n",
       "      <td>10.450156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG_SALE_TO_LIST_MOM</th>\n",
       "      <td>280191</td>\n",
       "      <td>9.046877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_LIST_PPSF</th>\n",
       "      <td>265885</td>\n",
       "      <td>8.584961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NEW_LISTINGS</th>\n",
       "      <td>261312</td>\n",
       "      <td>8.437307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFF_MARKET_IN_TWO_WEEKS_MOM</th>\n",
       "      <td>253685</td>\n",
       "      <td>8.191044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_LIST_PRICE</th>\n",
       "      <td>246676</td>\n",
       "      <td>7.964736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_PPSF_MOM</th>\n",
       "      <td>190091</td>\n",
       "      <td>6.137706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_DOM_MOM</th>\n",
       "      <td>188795</td>\n",
       "      <td>6.095860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOLD_ABOVE_LIST_MOM</th>\n",
       "      <td>181536</td>\n",
       "      <td>5.861480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PENDING_SALES</th>\n",
       "      <td>166909</td>\n",
       "      <td>5.389199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_SALE_PRICE_MOM</th>\n",
       "      <td>159668</td>\n",
       "      <td>5.155400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOMES_SOLD_MOM</th>\n",
       "      <td>159492</td>\n",
       "      <td>5.149717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AVG_SALE_TO_LIST</th>\n",
       "      <td>130912</td>\n",
       "      <td>4.226919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>OFF_MARKET_IN_TWO_WEEKS</th>\n",
       "      <td>93156</td>\n",
       "      <td>3.007844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_PPSF</th>\n",
       "      <td>36690</td>\n",
       "      <td>1.184656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_DOM</th>\n",
       "      <td>34307</td>\n",
       "      <td>1.107713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOLD_ABOVE_LIST</th>\n",
       "      <td>28242</td>\n",
       "      <td>0.911885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MEDIAN_SALE_PRICE</th>\n",
       "      <td>5303</td>\n",
       "      <td>0.171225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HOMES_SOLD</th>\n",
       "      <td>5128</td>\n",
       "      <td>0.165574</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             num_missing  pct_missing\n",
       "CITY                             3097102   100.000000\n",
       "MONTHS_OF_SUPPLY                 3097102   100.000000\n",
       "MONTHS_OF_SUPPLY_MOM             3097102   100.000000\n",
       "MONTHS_OF_SUPPLY_YOY             3097102   100.000000\n",
       "PRICE_DROPS_YOY                  3097102   100.000000\n",
       "PRICE_DROPS                      3097102   100.000000\n",
       "PRICE_DROPS_MOM                  3097102   100.000000\n",
       "INVENTORY_YOY                     676963    21.857950\n",
       "MEDIAN_LIST_PPSF_YOY              617045    19.923302\n",
       "NEW_LISTINGS_YOY                  615224    19.864506\n",
       "MEDIAN_LIST_PRICE_YOY             599623    19.360777\n",
       "PENDING_SALES_YOY                 557389    17.997115\n",
       "AVG_SALE_TO_LIST_YOY              527954    17.046710\n",
       "OFF_MARKET_IN_TWO_WEEKS_YOY       494734    15.974094\n",
       "INVENTORY_MOM                     486490    15.707910\n",
       "MEDIAN_DOM_YOY                    449354    14.508854\n",
       "MEDIAN_PPSF_YOY                   446351    14.411892\n",
       "SOLD_ABOVE_LIST_YOY               438163    14.147516\n",
       "MEDIAN_LIST_PPSF_MOM              425199    13.728931\n",
       "NEW_LISTINGS_MOM                  421046    13.594838\n",
       "MEDIAN_SALE_PRICE_YOY             417409    13.477406\n",
       "HOMES_SOLD_YOY                    417226    13.471497\n",
       "MEDIAN_LIST_PRICE_MOM             406937    13.139283\n",
       "INVENTORY                         340390    10.990597\n",
       "PENDING_SALES_MOM                 323652    10.450156\n",
       "AVG_SALE_TO_LIST_MOM              280191     9.046877\n",
       "MEDIAN_LIST_PPSF                  265885     8.584961\n",
       "NEW_LISTINGS                      261312     8.437307\n",
       "OFF_MARKET_IN_TWO_WEEKS_MOM       253685     8.191044\n",
       "MEDIAN_LIST_PRICE                 246676     7.964736\n",
       "MEDIAN_PPSF_MOM                   190091     6.137706\n",
       "MEDIAN_DOM_MOM                    188795     6.095860\n",
       "SOLD_ABOVE_LIST_MOM               181536     5.861480\n",
       "PENDING_SALES                     166909     5.389199\n",
       "MEDIAN_SALE_PRICE_MOM             159668     5.155400\n",
       "HOMES_SOLD_MOM                    159492     5.149717\n",
       "AVG_SALE_TO_LIST                  130912     4.226919\n",
       "OFF_MARKET_IN_TWO_WEEKS            93156     3.007844\n",
       "MEDIAN_PPSF                        36690     1.184656\n",
       "MEDIAN_DOM                         34307     1.107713\n",
       "SOLD_ABOVE_LIST                    28242     0.911885\n",
       "MEDIAN_SALE_PRICE                   5303     0.171225\n",
       "HOMES_SOLD                          5128     0.165574"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate the number of missing values in each column\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "# Calculate the percentage of missing values\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "\n",
    "# Create a report\n",
    "missing_report = pd.DataFrame({\n",
    "    'num_missing': missing_values,\n",
    "    'pct_missing': missing_percentage\n",
    "})\n",
    "\n",
    "# Show columns with at least one missing value, sorted by percentage\n",
    "print(\"--- Missing Values Report ---\")\n",
    "display(missing_report[missing_report['num_missing'] > 0].sort_values(by='pct_missing', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5e58b58-3bd9-4fa9-bbbc-e6d309ba1847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (3097102, 58)\n"
     ]
    }
   ],
   "source": [
    "# --- Create a copy to work on ---\n",
    "df_cleaned = df.copy()\n",
    "print(f\"Original DataFrame shape: {df_cleaned.shape}\")\n",
    "\n",
    "# Ensure data is sorted for time-series operations\n",
    "df_cleaned.sort_values(by=['ZIP_CODE', 'PERIOD_BEGIN'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de26a784-53dc-45b9-8d77-59a674ef9a59",
   "metadata": {},
   "source": [
    "### 5.1. Drop 100% Missing Columns\n",
    "It does not give any value to our case study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "083f9f2f-06c6-4d09-852a-ec7befdf6fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Dropped 7 unusable columns.\n"
     ]
    }
   ],
   "source": [
    "# === Step 1: Drop Unusable Columns (100% Missing) ===\n",
    "columns_to_drop = [\n",
    "    'CITY', 'MONTHS_OF_SUPPLY', 'MONTHS_OF_SUPPLY_MOM', 'MONTHS_OF_SUPPLY_YOY',\n",
    "    'PRICE_DROPS', 'PRICE_DROPS_MOM', 'PRICE_DROPS_YOY'\n",
    "]\n",
    "df_cleaned.drop(columns=columns_to_drop, inplace=True, errors='ignore')\n",
    "print(f\"\\nStep 1: Dropped {len(columns_to_drop)} unusable columns.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671cab20-5d7e-4b30-9d71-67c6b66bf0a0",
   "metadata": {},
   "source": [
    "### 5.2. Reconstruct Derived Metrics (YOY)\n",
    "For all Year-over-Year (YOY) and Month-over-Month (MOM) columns, we will first attempt to recalculate them from their raw source columns. This is the most accurate way to fill them in.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bf16dd6-88b7-4138-874d-c0b0e596d849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Reconstructed all YOY metrics.\n"
     ]
    }
   ],
   "source": [
    "# === Step 2: Reconstruct All Derived Metrics (YOY & MOM) ===\n",
    "# Dictionary of {derived_metric: raw_metric}\n",
    "reconstruction_map = {\n",
    "    # Year-over-Year (shift 4 quarters)\n",
    "    'MEDIAN_SALE_PRICE_YOY': 'MEDIAN_SALE_PRICE',\n",
    "    'HOMES_SOLD_YOY': 'HOMES_SOLD',\n",
    "    'PENDING_SALES_YOY': 'PENDING_SALES',\n",
    "    'NEW_LISTINGS_YOY': 'NEW_LISTINGS',\n",
    "    'INVENTORY_YOY': 'INVENTORY',\n",
    "    'MEDIAN_DOM_YOY': 'MEDIAN_DOM',\n",
    "    'MEDIAN_LIST_PRICE_YOY': 'MEDIAN_LIST_PRICE',\n",
    "    'MEDIAN_PPSF_YOY': 'MEDIAN_PPSF',\n",
    "    'MEDIAN_LIST_PPSF_YOY': 'MEDIAN_LIST_PPSF',\n",
    "    'AVG_SALE_TO_LIST_YOY': 'AVG_SALE_TO_LIST',\n",
    "    'SOLD_ABOVE_LIST_YOY': 'SOLD_ABOVE_LIST',\n",
    "}\n",
    "\n",
    "for derived_col, raw_col in reconstruction_map.items():\n",
    "    if raw_col in df_cleaned.columns and derived_col in df_cleaned.columns:\n",
    "        shift_period = 4 if '_YOY' in derived_col else 1\n",
    "        \n",
    "        # Get the value from the previous period\n",
    "        last_period_val = df_cleaned.groupby('ZIP_CODE')[raw_col].shift(shift_period)\n",
    "        \n",
    "        # Recalculate the percentage change\n",
    "        recomputed_val = ((df_cleaned[raw_col] - last_period_val) / last_period_val) * 100\n",
    "        \n",
    "        # Fill missing values in the derived column with our calculation\n",
    "        df_cleaned[derived_col] = df_cleaned[derived_col].fillna(recomputed_val)\n",
    "\n",
    "print(\"Step 2: Reconstructed all YOY metrics.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d83a9-3076-4481-945b-6a1901159ab0",
   "metadata": {},
   "source": [
    "### 5.3 Drop MoM data:\n",
    "A _MOM column would imply a comparison to the previous month, but that data doesn't exist. Instead, the comparison is to the previous quarter. This makes the _MOM label misleading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "265e4031-db6c-4862-9ace-c2a54e1c321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully dropped 12 misleading '_MOM' columns.\n",
      "Index(['PERIOD_BEGIN', 'PERIOD_END', 'PERIOD_DURATION', 'REGION_TYPE',\n",
      "       'REGION_TYPE_ID', 'TABLE_ID', 'IS_SEASONALLY_ADJUSTED', 'STATE',\n",
      "       'STATE_CODE', 'PROPERTY_TYPE', 'PROPERTY_TYPE_ID', 'MEDIAN_SALE_PRICE',\n",
      "       'MEDIAN_SALE_PRICE_YOY', 'MEDIAN_LIST_PRICE', 'MEDIAN_LIST_PRICE_YOY',\n",
      "       'MEDIAN_PPSF', 'MEDIAN_PPSF_YOY', 'MEDIAN_LIST_PPSF',\n",
      "       'MEDIAN_LIST_PPSF_YOY', 'HOMES_SOLD', 'HOMES_SOLD_YOY', 'PENDING_SALES',\n",
      "       'PENDING_SALES_YOY', 'NEW_LISTINGS', 'NEW_LISTINGS_YOY', 'INVENTORY',\n",
      "       'INVENTORY_YOY', 'MEDIAN_DOM', 'MEDIAN_DOM_YOY', 'AVG_SALE_TO_LIST',\n",
      "       'AVG_SALE_TO_LIST_YOY', 'SOLD_ABOVE_LIST', 'SOLD_ABOVE_LIST_YOY',\n",
      "       'OFF_MARKET_IN_TWO_WEEKS', 'OFF_MARKET_IN_TWO_WEEKS_YOY',\n",
      "       'PARENT_METRO_REGION', 'PARENT_METRO_REGION_METRO_CODE', 'LAST_UPDATED',\n",
      "       'ZIP_CODE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Find all columns that contain '_MOM' in their name\n",
    "mom_columns = [col for col in df_cleaned.columns if '_MOM' in col]\n",
    "\n",
    "# Drop these columns from the dataframe\n",
    "df_cleaned.drop(columns=mom_columns, inplace=True)\n",
    "\n",
    "print(f\"Successfully dropped {len(mom_columns)} misleading '_MOM' columns.\")\n",
    "print(df_cleaned.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7cbf56-d0a9-4b25-8aa2-9ba3e336fdf9",
   "metadata": {},
   "source": [
    "### 5.4. Impute Secondary Metrics: \n",
    "For important raw metrics with moderate (5-15%) missingness like INVENTORY or NEW_LISTINGS, we'll use a time-series forward-fill (ffill). This assumes that if data for a period is missing, market conditions were similar to the last known period for that same zip code—a very reasonable approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "240f2948-7451-4553-b942-ca702c90e520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Imputed secondary metrics using forward-fill.\n"
     ]
    }
   ],
   "source": [
    "# === Step 4: Impute Secondary Raw Metrics (5-15% Missing) ===\n",
    "secondary_cols_to_impute = [\n",
    "    'PENDING_SALES', 'MEDIAN_LIST_PRICE', 'NEW_LISTINGS',\n",
    "    'MEDIAN_LIST_PPSF', 'INVENTORY'\n",
    "]\n",
    "# Use forward-fill within each zip code group\n",
    "df_cleaned[secondary_cols_to_impute] = df_cleaned.groupby('ZIP_CODE')[secondary_cols_to_impute].ffill()\n",
    "print(\"Step 4: Imputed secondary metrics using forward-fill.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb31e2bf-8df4-4ec8-959c-ffd57136291b",
   "metadata": {},
   "source": [
    "### 5.5. Final Cleanup:\n",
    "For the most essential raw metrics with very low (<5%) missingness (MEDIAN_SALE_PRICE, MEDIAN_DOM), we will drop the few remaining rows. This ensures our core analysis is based purely on observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52b47e25-bd33-42c6-a651-cf44d02ff5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Dropped remaining rows with missing essential data.\n",
      "\n",
      "Cleaned DataFrame shape: (3062620, 39)\n",
      "\n",
      "--- Remaining Missing Values Report ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OFF_MARKET_IN_TWO_WEEKS_YOY    482119\n",
       "INVENTORY_YOY                  400940\n",
       "MEDIAN_LIST_PPSF_YOY           344125\n",
       "NEW_LISTINGS_YOY               333122\n",
       "MEDIAN_LIST_PRICE_YOY          323604\n",
       "SOLD_ABOVE_LIST_YOY            227173\n",
       "PENDING_SALES_YOY              222673\n",
       "AVG_SALE_TO_LIST_YOY           174714\n",
       "AVG_SALE_TO_LIST               124422\n",
       "OFF_MARKET_IN_TWO_WEEKS         90536\n",
       "MEDIAN_PPSF_YOY                 78662\n",
       "PENDING_SALES                   78367\n",
       "INVENTORY                       72565\n",
       "MEDIAN_DOM_YOY                  50395\n",
       "MEDIAN_SALE_PRICE_YOY           43345\n",
       "HOMES_SOLD_YOY                  43314\n",
       "MEDIAN_PPSF                     30511\n",
       "SOLD_ABOVE_LIST                 28133\n",
       "NEW_LISTINGS                    25338\n",
       "MEDIAN_LIST_PPSF                17447\n",
       "MEDIAN_LIST_PRICE               14083\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Step 5: Final Cleanup of Essential Metrics (<5% Missing) ===\n",
    "# These are the absolute minimum columns needed for a valid record\n",
    "essential_columns = [\n",
    "    'MEDIAN_SALE_PRICE', 'HOMES_SOLD', 'MEDIAN_DOM'\n",
    "]\n",
    "df_cleaned.dropna(subset=essential_columns, inplace=True)\n",
    "print(\"Step 5: Dropped remaining rows with missing essential data.\")\n",
    "\n",
    "\n",
    "# === Final Report ===\n",
    "print(f\"\\nCleaned DataFrame shape: {df_cleaned.shape}\")\n",
    "print(\"\\n--- Remaining Missing Values Report ---\")\n",
    "missing_report = df_cleaned.isnull().sum()\n",
    "display(missing_report[missing_report > 0].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab45d2-c4d8-41e5-a166-4299d0f15b03",
   "metadata": {},
   "source": [
    "### 5.6. Saved the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3996fa95-a6dc-42c7-9ae2-5876709c4b7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset successfully saved to: data/cleaned_data/redfin_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path for the cleaned data\n",
    "cleaned_data_path = 'data/cleaned_data/redfin_cleaned.csv'\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(cleaned_data_path), exist_ok=True)\n",
    "\n",
    "# Save the cleaned dataframe to a CSV file\n",
    "df_cleaned.to_csv(cleaned_data_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset successfully saved to: {cleaned_data_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60083d0-893f-49d8-9cae-ba0f734cce33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
